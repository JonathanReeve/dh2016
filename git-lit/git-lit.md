%Git-Lit: an Application of Distributed Version Control Technology toward the Creation of 50,000 Digital Scholarly Editions
%Jonathan Reeve
%Columbia University

Distributed version control technologies, the most popular protocols of which are `git`, `subversion`, and `mercurial`, have long been popular among computer programmers for their abilities to track changes in a codebase and foster collaboration among coders. When combined with code management platforms such as GitHub, BitBucket, or GitLab, they become even more powerul, enabling sophisticated bug tracking, project planning, and open-source code publication. Although these technologies have not yet been in widespread use in the humanities, their potential for use with corpus creation and textual editing is far-reaching. This paper describes Git-Lit, an open-source, community-centered initiative to parse, version control, and publish to GitHub roughly 50,000 scanned public-domain books recently made available by the British Library, thereby facilitating decentralized, open-access, and democratic scholarly editing. 

The Git-Lit initiative addresses these problems: 

1. **Electronic texts are difficult to edit.** Traditional text repositories like Project Gutenberg and the Oxford Text Archive maintain central, canonical versions of their texts that, in most cases, are virtually immutable. If a reader spots an OCR error in an ebook, he or she must rely on contacting the publisher to propose a correction. Even with a dedicated team such as Project Gutenberg's Distributed Proofreaders, the process of releasing a corrected edition may take months or years. Git-Lit aims to radically streamline the editing of an electronic text by decentralizing the text base with distributed version control. In a decentralized model, no single text may be considered unquestionably canonical, although _de facto_ canonicity might be democratically achieved through repository voting mechanisms such as GitHub's stars. 

2. **Electronic texts often lack an editorial history.** Owing, in some cases, to the age of an electronic text, its editorial provenance is often lost. Many of the Project Gutenberg editions, for instance, are transcribed from unknown print editions, and the history of their revisions is opaque. Version control mitigates these problems by recording every edit, editor, and edition in the history of the text. When two editions diverge, git provides sophisticated tools for analyzing the differences between these editions. 

3. **Textual corpora are difficult to assemble.** With some exceptions, notably the NLTK corpus module, downloading a text corpus involves compiling texts from diverse and heterogeneous sources. A would-be text analyst must click through a series of web pages to find the corpus he or she wants, and then either download a .zip file that must be expanded, or email the corpus creator for a copy. This can be a labor-intensive process that is not easily scriptable or automated. Git provides an easy way to solve these problems: by making texts available through the git protocol on GitHub, anyone that wishes to download a text corpus can simply run the command `git clone` followed by the repository URL. Parent repositories can then be assembled for collections of texts using git submodules.  Parent corpus repository might be created for nineteenth-century _Bildungsromane_, for instance, and that repository would contain pointers to individual text repositories. These categories would not necessarily be mutually exclusive and would allow for arbitrary curation of custom corpora. 

4. **ALTO XML is not very human-readable.** ALTO XML, the OCR output format used by the British Library, the Library of Congress, and others, is extremely verbose. It encodes the location of each word on the page, and often gives the OCR certainty for each word. While this format is useful for archival purposes, plain text editions are more useful for most brands of computational text analysis. Git-Lit parses the British Library's ALTO XML, and creates markdown versions of each text that are easily edited. Each of these texts is then programmatically converted via Jekyll into a web page hosted on github.io, creating readable web editions for each of the 50,000 texts in the corpus. 

The Git-Lit scripts work by first parsing the XML metadata included with each text. This metadata is used to programmatically generate a repository name and a `README.md` file that describes the text, a document which GitHub will automatically render into a web page at the repository root. This file, along with standard `CONTRIBUTING` and `LICENSE` files (each text is released under the GPLv3), is then committed to local git repositories, initiating version control of the texts. The resulting local repository is then uploaded to GitHub via Python bindings to the GitHub API. Parent repositories are then created for each collection of texts based on the their associated Library of Congress subjects. This enables a text analyst interested in 19th century poetry, for instance, to download all of the British Library's released etexts in this genre simply by running `git clone https://github.com/git-lit/19th-century-poetry.git && git submodule update --init --recursive`. 

Since British Library texts are not the only ones being published to git-based platforms like GitHub---notable version-controlled corpora on GitHub include TEI texts from the Text Creation Partnership and the early modern corpus Shakespeare His Contemporaries---git provides a common protocol for sharing, modifying, and distributing texts and textual corpora. Anyone may aggregate these corpora into parent repositories using git submodules. The Git-Lit projects plans to soon launch a web application that will routinely scrape GitHub for any textual corpus, thereby automating the process of indexing available corpora. 

The corpora made available by Git-Lit are currently in the process of being integrated into DHBox, the cloud-based Digital Humanities software suite. Soon, these corpora and many others will be available for download by selecting them from a web-based interface, where they will then available for analysis using pre-installed versions of the Python NLTK, R, and other textual analytic tools.

This paper will discuss how these technologies might be used by other digital humanities projects involved in the creation or analysis of large text corpora. It will discuss some of the storage and computation limitations of electronicaly publishing texts on code repositories, and some of the problems encountered by the Git-Lit project. Finally, it will discuss pedagogical uses of collaborative digital editing, such as classroom compilation of anthologies or digital scholarly editions.
